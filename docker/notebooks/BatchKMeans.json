{"paragraphs":[{"text":"%md\nRun BatchKMeans to iterate over the data set and find the optimal number of clusters.\n\nUsage: Usage: Set the following parameters in the paragraph below by running nwintrusion/bin/app-install.sh –start-only batch-k-means\nTOPIC_TO_READ_FROM\nBROKER\nMICROBATCH_DURATION\nFROM_CLUSTER_COUNT\nTO_CLUSTER_COUNT\nINCREMENT\n\nThis will run on streaming data from the specified Kafka topic in microbatches of the specified duration.\nIt's recommended that the duration is long enough to have sufficient amount of data for cluster number optimization.\nEvery run of the microbatch will iterate through cluster numbers <from cluster count> to <to cluster count>\nin steps of <increment> and record the mean squared error. e.g. if <from cluster count> = 10 and <to cluster count>\n= 40 and <increment> = 10, then each microbatch will run for cluster numbers = [10, 20, 30, 40] and\nrecord the mean squared error. We need to pick up the one after which the error starts to go up.","dateUpdated":"2017-04-04T19:36:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Run BatchKMeans to iterate over the data set and find the optimal number of clusters.</p>\n<p>Usage: Usage: Set the following parameters in the paragraph below by running nwintrusion/bin/app-install.sh –start-only batch-k-means<br/>TOPIC_TO_READ_FROM<br/>BROKER<br/>MICROBATCH_DURATION<br/>FROM_CLUSTER_COUNT<br/>TO_CLUSTER_COUNT<br/>INCREMENT</p>\n<p>This will run on streaming data from the specified Kafka topic in microbatches of the specified duration.<br/>It&rsquo;s recommended that the duration is long enough to have sufficient amount of data for cluster number optimization.<br/>Every run of the microbatch will iterate through cluster numbers <from cluster count> to <to cluster count><br/>in steps of <increment> and record the mean squared error. e.g. if <from cluster count> = 10 and <to cluster count><br/>= 40 and <increment> = 10, then each microbatch will run for cluster numbers = [10, 20, 30, 40] and<br/>record the mean squared error. We need to pick up the one after which the error starts to go up.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1491334601909_-1478771732","id":"20170324-032341_1993876324","dateCreated":"2017-04-04T19:36:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:296"},{"text":"import org.apache.spark.streaming.Seconds\n\nval topicToReadFrom = <TOPIC_TO_READ_FROM>\nval broker = <BROKER>\nval microbatchDuration = <MICROBATCH_DURATION>\nval fromClusterCount = <FROM_CLUSTER_COUNT>\nval toClusterCount = <TO_CLUSTER_COUNT>\nval increment = <INCREMENT>","user":"anonymous","dateUpdated":"2017-04-05T23:39:32+0000","config":{"tableHide":true,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.streaming.Seconds\n\ntopicToReadFrom: Array[String] = Array(nwout)\n\nbroker: String = broker-0.kafka.mesos:10036\n\nmicrobatchDuration: org.apache.spark.streaming.Duration = 60000 ms\n\nfromClusterCount: Int = 10\n\ntoClusterCount: Int = 100\n\nincrement: Int = 10\n"}]},"apps":[],"jobName":"paragraph_1491334601914_-1479156481","id":"20170404-192636_1617220607","dateCreated":"2017-04-04T19:36:41+0000","dateStarted":"2017-04-05T23:39:32+0000","dateFinished":"2017-04-05T23:39:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:297"},{"text":"import org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.ByteArrayDeserializer\n\nimport org.apache.spark.mllib.linalg.{ Vector, Vectors }\nimport org.apache.spark.mllib.clustering.{ KMeans, KMeansModel }\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.streaming.kafka010.KafkaUtils\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n\nimport java.nio.charset.Charset\n\nif (fromClusterCount > toClusterCount) sys.error(s\"Invalid cluster count range provided [$fromClusterCount,$toClusterCount]\")\n\nval streamingContext = new StreamingContext(sc, microbatchDuration)\n\n/**\n* Train a KMean model using normalized data.\n*/\ndef trainModel(normalizedData: RDD[Vector], noOfClusters: Int): KMeansModel = {\n    val kmeans = new KMeans()\n    kmeans.setK(noOfClusters)\n    kmeans.run(normalizedData)\n}\n\ndef normalize(rdd: RDD[Vector]): RDD[Vector] = {\n    new StandardScaler().fit(rdd).transform(rdd)\n}\n\ndef distanceToCentroid(model: KMeansModel, vec: Vector): Double = {\n    val predictedCluster: Int = model.predict(vec)\n    val centroid: Vector = model.clusterCenters(predictedCluster)\n    Vectors.sqdist(centroid, vec)\n}\n\nval kafkaParams = Map[String, Object](\n  \"bootstrap.servers\"    -> broker,\n  \"key.deserializer\"     -> classOf[ByteArrayDeserializer],\n  \"value.deserializer\"   -> classOf[ByteArrayDeserializer],\n  \"group.id\"             -> \"example\",\n  \"auto.offset.reset\"    -> \"latest\",\n  \"enable.auto.commit\"   -> (false: java.lang.Boolean)\n)\n\nval stream: DStream[ConsumerRecord[Array[Byte], Array[Byte]]] = \n  KafkaUtils.createDirectStream[Array[Byte], Array[Byte]](\n    streamingContext,\n    PreferConsistent,\n    Subscribe[Array[Byte], Array[Byte]](topicToReadFrom, kafkaParams)\n  )\n\nval connectionData: DStream[Vector] = stream.map { record =>\n  val bytes: Array[Byte] = record.value\n  val CHARSET = Charset.forName(\"UTF-8\")\n  val arr = new String(bytes, CHARSET).split(\"/\")\n  val (label, connectionInfo) = (arr(0), arr(1))\n  Vectors.dense(connectionInfo.split(\",\").map(_.toDouble))\n}\n\nconnectionData.foreachRDD { rdd =>\n  if (!rdd.isEmpty()) {\n    val normalizedData: RDD[Vector] = normalize(rdd).cache()\n\n    (fromClusterCount to toClusterCount by increment).foreach { noOfClusters =>\n      val trainedModel: KMeansModel = trainModel(normalizedData, noOfClusters)\n      val clusteringScore: Double = normalizedData.map(distanceToCentroid(trainedModel, _)).mean()\n      println(s\"No of clusters = $noOfClusters, score = $clusteringScore\")\n    }\n  }\n}\n\nsys.ShutdownHookThread {\n  streamingContext.stop(stopSparkContext=false, stopGracefully=true)\n}\n","user":"anonymous","dateUpdated":"2017-04-05T23:39:38+0000","config":{"tableHide":true,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.kafka.clients.consumer.ConsumerRecord\n\nimport org.apache.kafka.common.serialization.ByteArrayDeserializer\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\nimport org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n\nimport org.apache.spark.mllib.feature.StandardScaler\n\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.SparkConf\n\nimport org.apache.spark.streaming.dstream.DStream\n\nimport org.apache.spark.streaming.StreamingContext\n\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nimport org.apache.spark.streaming.kafka010.KafkaUtils\n\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n\nimport java.nio.charset.Charset\n\nstreamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2969fa5e\n\ntrainModel: (normalizedData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector], noOfClusters: Int)org.apache.spark.mllib.clustering.KMeansModel\n\nnormalize: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector])org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector]\n\ndistanceToCentroid: (model: org.apache.spark.mllib.clustering.KMeansModel, vec: org.apache.spark.mllib.linalg.Vector)Double\n\nkafkaParams: scala.collection.immutable.Map[String,Object] = Map(key.deserializer -> class org.apache.kafka.common.serialization.ByteArrayDeserializer, auto.offset.reset -> latest, group.id -> example, bootstrap.servers -> broker-0.kafka.mesos:10036, enable.auto.commit -> false, value.deserializer -> class org.apache.kafka.common.serialization.ByteArrayDeserializer)\n\nstream: org.apache.spark.streaming.dstream.DStream[org.apache.kafka.clients.consumer.ConsumerRecord[Array[Byte],Array[Byte]]] = org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@33a04f76\n\nconnectionData: org.apache.spark.streaming.dstream.DStream[org.apache.spark.mllib.linalg.Vector] = org.apache.spark.streaming.dstream.MappedDStream@3eb0381d\n\nres6: scala.sys.ShutdownHookThread = Thread[shutdownHook2,5,main]\n"}]},"apps":[],"jobName":"paragraph_1491334601915_-1479541229","id":"20170324-032400_1896681503","dateCreated":"2017-04-04T19:36:41+0000","dateStarted":"2017-04-05T23:39:38+0000","dateFinished":"2017-04-05T23:39:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:298"},{"text":"streamingContext.start()\nstreamingContext.awaitTermination()","user":"anonymous","dateUpdated":"2017-04-05T23:40:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/text","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491334601916_-1481464974","id":"20170404-145629_299805499","dateCreated":"2017-04-04T19:36:41+0000","dateStarted":"2017-04-05T23:40:39+0000","status":"RUNNING","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:299"},{"dateUpdated":"2017-04-04T19:36:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/text","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491334601916_-1481464974","id":"20170324-032813_305496147","dateCreated":"2017-04-04T19:36:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:300"}],"name":"FDP Sample Apps/BatchKMeans","id":"2CCMQVH7D","angularObjects":{"2CFC3PVP9:shared_process":[],"2CEZXHM6P:shared_process":[],"2CCEGDB3R:shared_process":[],"2CDWR435J:shared_process":[],"2CD24A5JD:shared_process":[],"2CD4PZBJ3:shared_process":[],"2CE7E95SM:shared_process":[],"2CEXB4C5Y:shared_process":[],"2CEZSP4AS:shared_process":[],"2CDYWUGX7:shared_process":[],"2CCRPQXAR:shared_process":[],"2CCVFXHFA:shared_process":[],"2CFG46E6X:shared_process":[],"2CE5UGXWM:shared_process":[],"2CEFDKAEH:shared_process":[],"2CFGGF2W7:shared_process":[],"2CD6HV5V5:shared_process":[],"2CF4VH8CK:shared_process":[],"2CCYWWDPY:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}