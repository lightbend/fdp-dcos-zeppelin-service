{"paragraphs":[{"text":"%md\nRun SparkClustering to iterate over the data set in micro batches and generate clustering information with anomalies\n\nUsage: Set the following parameters in the paragraph below by running nwintrusion/bin/app-install.sh --start-only anomaly-detection\nNO_OF_CLUSTERS\nTOPIC_TO_READ_FROM\nBROKER\nCLUSTER_TOPIC\nDURATION\n\nThis will run on streaming data from the specified Kafka topic in microbatches of the specified duration.\nThe result will tag every data point with a cluster number and also flag as anomaly for the anomalous data point.\nThe model for anomaly detection is based on the distance of the point from the nearest centroid being 3 standard\ndeviations away from the mean.","dateUpdated":"2017-04-04T19:22:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Run SparkClustering to iterate over the data set in micro batches and generate clustering information with anomalies</p>\n<p>Usage: Set the following parameters in the paragraph below by running nwintrusion/bin/app-install.sh &ndash;start-only anomaly-detection<br/>NO_OF_CLUSTERS<br/>TOPIC_TO_READ_FROM<br/>BROKER<br/>CLUSTER_TOPIC<br/>DURATION</p>\n<p>This will run on streaming data from the specified Kafka topic in microbatches of the specified duration.<br/>The result will tag every data point with a cluster number and also flag as anomaly for the anomalous data point.<br/>The model for anomaly detection is based on the distance of the point from the nearest centroid being 3 standard<br/>deviations away from the mean.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1491333282708_1015706985","id":"20170404-145214_264073144","dateCreated":"2017-04-04T19:14:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1525","user":"anonymous","dateFinished":"2017-04-04T19:22:57+0000","dateStarted":"2017-04-04T19:22:56+0000"},{"text":"import org.apache.spark.streaming.{ StreamingContext, Seconds }\n\n// Set the following 5 parameters\nval noOfClusters = <NO_OF_CLUSTERS>\nval topicToReadFrom = <TOPIC_TO_READ_FROM>\nval broker = <BROKER>\nval clusterTopic = <CLUSTER_TOPIC>\nval duration = <DURATION>","user":"anonymous","dateUpdated":"2017-04-04T19:23:03+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491333650751_-1026836270","id":"20170404-192050_1739523649","dateCreated":"2017-04-04T19:20:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1961","dateFinished":"2017-04-04T19:23:03+0000","dateStarted":"2017-04-04T19:23:03+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.streaming.{StreamingContext, Seconds}\n\nnoOfClusters: Int = 30\n\ntopicToReadFrom: Array[String] = Array(nwout)\n\nbroker: String = broker-0.kafka.mesos:10071\n\nclusterTopic: String = nwcls\n\nduration: org.apache.spark.streaming.Duration = 1000 ms\n"}]}},{"text":"%spark\n\nimport org.apache.kafka.common.serialization.{ ByteArraySerializer, StringSerializer }\n\nimport org.apache.spark.mllib.linalg.{ Vectors, Vector }\nimport org.apache.spark.mllib.clustering.{ StreamingKMeans, StreamingKMeansModel }\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.StreamingContext\n\nimport java.io.Serializable\n\nimport org.apache.kafka.clients.producer.{ KafkaProducer, ProducerRecord, Callback, RecordMetadata }\n\nclass KafkaSink(createProducer: () => org.apache.kafka.clients.producer.KafkaProducer[Nothing, String]) extends Serializable {\n\n  lazy val producer = createProducer()\n\n  def send(topic: String, value: String): Unit = {\n    \n    val pr = new org.apache.kafka.clients.producer.ProducerRecord(topic, value)\n    val cb = new org.apache.kafka.clients.producer.Callback {\n      override def onCompletion(metadata: org.apache.kafka.clients.producer.RecordMetadata, ex: Exception) = \n        if (ex != null) ex.printStackTrace\n    }\n\n    producer.send(pr, cb)\n  }\n}\n\ndef createKafkaSink(broker: String) = {\n    val kafkaParams = new java.util.HashMap[String, Object]()\n    kafkaParams.put(\"key.serializer\", classOf[org.apache.kafka.common.serialization.ByteArraySerializer])\n    kafkaParams.put(\"value.serializer\", classOf[org.apache.kafka.common.serialization.StringSerializer])\n    kafkaParams.put(\"bootstrap.servers\", broker)\n    \n    val f = () => {\n       val producer = new org.apache.kafka.clients.producer.KafkaProducer[Nothing, String](kafkaParams)\n\n       sys.addShutdownHook {\n         producer.close()\n       }\n\n       producer\n     }\n     new KafkaSink(f)\n}\n\nobject KafkaPublisher extends Serializable {\n  /**\n   * Predict cluster from `labeledData` and publish to Kafka. \n   *\n   * The following elements are published separated by comma:\n   *\n   * a. predicted cluster number\n   * b. centroid of the cluster\n   * c. distance of the point from the centroid\n   * d. the label of the point\n   * e. if anomalous\n   * f. the point vector as a comma separated string\n   *\n   * For every microbatch, a separate record is written to the same topic, which contains\n   * the pattern : \"Centroids:/<centroid1>/<centroid2>/...\"\n   */ \n\n  def writeToKafka(labeledData: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)], skmodel:  org.apache.spark.mllib.clustering.StreamingKMeansModel, sink: KafkaSink, clusterTopic: String) = labeledData.foreachRDD { rdd =>\n\n    var currentHundredthFarthest: Double = 0.0d\n    if (rdd.count() > 0) {\n  \n      // for the RDD of the microbatch, get the mean distance to centroid for all the Vectors\n      val distancesToCentroid: org.apache.spark.rdd.RDD[(Int, Double)] = rdd.map { case (l, v) => \n        val (cluster, _, dist) = distanceToCentroid(skmodel, v)\n        (cluster, dist)\n      }\n\n      /**\n       * We compute the hundredth farthest point from centroid in each micro batch and\n       * delcare a data point anomalous if it's distance from the nearest centroid exceeds this value.\n       * We maintain the highest of the values that we get for each batch. This is not entirely\n       * foolproof a model, but this will improve with more iterations\n       */ \n      val hundredthFarthestDistanceToCentroid: Double = { \n        val t100 = rdd.map { case (l, v) => distanceToCentroid(skmodel, v)._3 }.top(100)\n        if (t100.isEmpty) 0.00 else t100.last\n      }\n      if (hundredthFarthestDistanceToCentroid > currentHundredthFarthest) \n        currentHundredthFarthest = hundredthFarthestDistanceToCentroid\n\n      (rdd.zipWithIndex).foreach { case ((label, vec), idx) =>\n  \n        // for each Vector in the RDD get the cluster membership, centroid and distance to centroid\n        val (predictedCluster, centroid, distanceToCentroidForVec) = distanceToCentroid(skmodel, vec) \n  \n        // anomalous if its distance is more than the hundredth farthest distance\n        // This strategy is taken from the book Advanced Analytics with Spark (http://shop.oreilly.com/product/0636920035091.do)\n        val isAnomalous = distanceToCentroidForVec > currentHundredthFarthest\n  \n        val message = \n          s\"\"\"$predictedCluster,$centroid,$distanceToCentroidForVec,$label,$isAnomalous,${vec.toArray.mkString(\",\")}\"\"\"\n  \n        // write the centroid record only for the first item in the microbatch\n        if (idx == 0) sink.send(clusterTopic, createClusterCenterRecord(skmodel))\n        sink.send(clusterTopic, message)\n      }\n    }\n  }\n  \n  def publishClusterInfoToKafka(labeledData: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)], model: org.apache.spark.mllib.clustering.StreamingKMeans, sink: KafkaSink, clusterTopic: String, broker: String) = {\n    writeToKafka(labeledData, model.latestModel, sink, clusterTopic)\n  }\n\n  /**\n   * Create a record consisting of cluster centroids separated by \"/\". The cluster centroids are in the \n   * order of the cluster numbers.\n   */ \n  def createClusterCenterRecord(skmodel:  org.apache.spark.mllib.clustering.StreamingKMeansModel): String = {\n    val centers: Array[org.apache.spark.mllib.linalg.Vector] = skmodel.clusterCenters\n    centers.foldLeft(s\"Centroids:\") { (a, e) =>\n      s\"$a/${e.toArray.mkString(\",\")}\"\n    }\n  }\n\n  private def distanceToCentroid(skmodel:  org.apache.spark.mllib.clustering.StreamingKMeansModel, vec: org.apache.spark.mllib.linalg.Vector): (Int, org.apache.spark.mllib.linalg.Vector, Double) = {\n    val predictedCluster: Int = skmodel.predict(vec)\n    val centroid: org.apache.spark.mllib.linalg.Vector = skmodel.clusterCenters(predictedCluster)\n    (predictedCluster, centroid, org.apache.spark.mllib.linalg.Vectors.sqdist(centroid, vec))\n  }\n\n  private def normalize(rdd: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)]): org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] = {\n    val labels: org.apache.spark.rdd.RDD[String] = rdd.map(_._1)\n    val data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = rdd.map(_._2)\n    labels.zip(new org.apache.spark.mllib.feature.StandardScaler().fit(data).transform(data))\n  }\n}\n","dateUpdated":"2017-04-04T19:23:07+0000","config":{"lineNumbers":true,"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.kafka.common.serialization.{ByteArraySerializer, StringSerializer}\n\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\n\nimport org.apache.spark.mllib.clustering.{StreamingKMeans, StreamingKMeansModel}\n\nimport org.apache.spark.mllib.feature.StandardScaler\n\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.streaming.dstream.DStream\n\nimport org.apache.spark.streaming.StreamingContext\n\nimport java.io.Serializable\n\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, Callback, RecordMetadata}\n\ndefined class KafkaSink\n\n\ncreateKafkaSink: (broker: String)KafkaSink\ndefined object KafkaPublisher\n"}]},"apps":[],"jobName":"paragraph_1491333282713_1013783240","id":"20170324-014043_420574488","dateCreated":"2017-04-04T19:14:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1526","user":"anonymous","dateFinished":"2017-04-04T19:23:09+0000","dateStarted":"2017-04-04T19:23:07+0000"},{"text":"import org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.{ ByteArrayDeserializer, StringDeserializer}\n\nimport org.apache.spark.mllib.clustering.{ StreamingKMeans, StreamingKMeansModel }\nimport org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.mllib.linalg.{ Vector, Vectors }\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.StreamingContext\n\nimport java.nio.charset.Charset\n\ndef projectToLowerDimension: RDD[(String, Vector)] => RDD[(String, Vector)] = { rdd =>\n    if (rdd.isEmpty) rdd else {\n      // reduce to 2 dimensions\n      val pca = new PCA(2).fit(rdd.map(_._2))\n    \n      // Project vectors to the linear space spanned by the top 2 principal\n      // components, keeping the label\n      rdd.map(p => (p._1, pca.transform(p._2)))\n    }\n}\n\ndef normalize: RDD[(String, Vector)] => RDD[(String, Vector)] = { rdd =>\n    if (rdd.isEmpty) rdd else {\n      val labels: RDD[String] = rdd.map(_._1)\n      val data: RDD[Vector] = rdd.map(_._2)\n      labels.zip(new StandardScaler().fit(data).transform(data))\n    }\n}\n\nval streamingContext = new StreamingContext(sc, duration)\n// streamingContext.checkpoint(\"checkpoint\")\n\nval kafkaParams = Map[String, Object](\n  \"bootstrap.servers\"    -> broker,\n  \"key.deserializer\"     -> classOf[ByteArrayDeserializer],\n  \"value.deserializer\"   -> classOf[ByteArrayDeserializer],\n  \"group.id\"             -> s\"example${System.currentTimeMillis}\",\n  \"auto.offset.reset\"    -> \"latest\",\n  \"enable.auto.commit\"   -> (true: java.lang.Boolean)\n)\n\n// get the data from kafka\nval stream: DStream[ConsumerRecord[Array[Byte], Array[Byte]]] = \n  KafkaUtils.createDirectStream[Array[Byte], Array[Byte]](\n    streamingContext,\n    PreferConsistent,\n    Subscribe[Array[Byte], Array[Byte]](topicToReadFrom, kafkaParams)\n  )\n\n// label and vectorize the value\nval transformed: DStream[(String, Vector)] = stream.map { record =>\n  val bytes: Array[Byte] = record.value\n  val CHARSET = Charset.forName(\"UTF-8\")\n  val arr = new String(bytes, CHARSET).split(\"/\")\n  val (label, value) = (arr(0), arr(1))\n  val vector = Vectors.dense(value.split(\",\").map(_.toDouble))\n  (label, vector)\n}\n\ntransformed.cache()\n\n// normalize\nval normalized: DStream[(String, Vector)] = transformed.transform(normalize)\n\n// project to lower dimension\nval projected: DStream[(String, Vector)] = normalized.transform(projectToLowerDimension)\n\n// get data only\nval data: DStream[Vector] = projected.map(_._2)\n","dateUpdated":"2017-04-04T19:23:13+0000","config":{"lineNumbers":true,"tableHide":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.kafka.clients.consumer.ConsumerRecord\n\nimport org.apache.kafka.common.serialization.{ByteArrayDeserializer, StringDeserializer}\n\nimport org.apache.spark.mllib.clustering.{StreamingKMeans, StreamingKMeansModel}\n\nimport org.apache.spark.mllib.feature.PCA\n\nimport org.apache.spark.mllib.feature.StandardScaler\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.SparkConf\n\nimport org.apache.spark.streaming.dstream.DStream\n\nimport org.apache.spark.streaming.kafka010._\n\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n\nimport org.apache.spark.streaming.StreamingContext\n\nimport java.nio.charset.Charset\n\nprojectToLowerDimension: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] => org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)]\n\nnormalize: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] => org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)]\n\nstreamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@424d8eb7\n\nkafkaParams: scala.collection.immutable.Map[String,Object] = Map(key.deserializer -> class org.apache.kafka.common.serialization.ByteArrayDeserializer, auto.offset.reset -> latest, group.id -> example1491333795265, bootstrap.servers -> broker-0.kafka.mesos:10071, enable.auto.commit -> true, value.deserializer -> class org.apache.kafka.common.serialization.ByteArrayDeserializer)\n\nstream: org.apache.spark.streaming.dstream.DStream[org.apache.kafka.clients.consumer.ConsumerRecord[Array[Byte],Array[Byte]]] = org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@7ec0009f\n\ntransformed: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)] = org.apache.spark.streaming.dstream.MappedDStream@47ca09\n\nres6: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)] = org.apache.spark.streaming.dstream.MappedDStream@47ca09\n\nnormalized: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)] = org.apache.spark.streaming.dstream.TransformedDStream@59fb77\n\nprojected: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)] = org.apache.spark.streaming.dstream.TransformedDStream@5e44cc70\n\ndata: org.apache.spark.streaming.dstream.DStream[org.apache.spark.mllib.linalg.Vector] = org.apache.spark.streaming.dstream.MappedDStream@7b00f704\n"}]},"apps":[],"jobName":"paragraph_1491333282715_1014552738","id":"20170323-205432_560258814","dateCreated":"2017-04-04T19:14:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1527","user":"anonymous","dateFinished":"2017-04-04T19:23:16+0000","dateStarted":"2017-04-04T19:23:13+0000"},{"text":"val decayFactor = 1      // assume stationary distribution\nval dimensions = 2       // our data is now a 2 dimension vector\nval weightPerCenter = 0.0\n\nval model = new StreamingKMeans()\n  .setDecayFactor(decayFactor)\n  .setK(noOfClusters)\n  .setRandomCenters(dimensions, weightPerCenter)\n\nmodel.trainOn(data)\n\nval skmodel = model.latestModel\n\nskmodel.clusterCenters.foreach(println)\nskmodel.clusterWeights.foreach(println)\n\n// cluster distribution on prediction\nval predictedClusters: DStream[Int] = model.predictOn(data)\n\n// prints counts per cluster \npredictedClusters.countByValue().print()\n\npredictedClusters.foreachRDD { rdd => println(rdd.count()) }\n\nsys.ShutdownHookThread {\n  streamingContext.stop(stopSparkContext=false, stopGracefully=true)\n}\n\nval sink = createKafkaSink(broker)\nval broadcastedSink = streamingContext.sparkContext.broadcast(sink)\n// publish cluster info to a kafka topic\nKafkaPublisher.publishClusterInfoToKafka(projected, model, broadcastedSink.value, clusterTopic, broker)","dateUpdated":"2017-04-04T19:23:19+0000","config":{"tableHide":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndecayFactor: Int = 1\n\ndimensions: Int = 2\n\nweightPerCenter: Double = 0.0\n\nmodel: org.apache.spark.mllib.clustering.StreamingKMeans = org.apache.spark.mllib.clustering.StreamingKMeans@2d0ff0a\n\nskmodel: org.apache.spark.mllib.clustering.StreamingKMeansModel = org.apache.spark.mllib.clustering.StreamingKMeansModel@64512373\n[0.6496302611379806,-1.2873253687844466]\n[1.0147984704794435,0.40277750632067055]\n[-0.644631903391192,-0.28958701617885996]\n[0.9157206429922232,-1.5061125251527852]\n[0.6977502509950515,0.7774991998823817]\n[-0.6546761365364097,0.1486391699712007]\n[0.4295501957877953,-0.2440661119169439]\n[1.4199027668876063,0.21774983654789304]\n[0.8433768793526613,-0.8135122556736286]\n[-0.08115352196328288,0.5071241139653296]\n[-0.6012259268575378,-0.8601755648418907]\n[-1.081099347220249,-0.23013862997076362]\n[-1.4669988638105824,-0.3300053472014135]\n[0.13297332554521724,0.01710758322804697]\n[0.25714782770455796,-0.5690609714483817]\n[0.4682583959420661,-0.3289435852380947]\n[-0.3242683017423009,-2.5561155028620672]\n[0.626650434883708,-0.9674093645122231]\n[1.3198883894766644,0.5771766377764465]\n[-0.7655263101293215,0.17093826173370086]\n[1.9573045684673651,1.047386692916962]\n[1.5303276623169146,1.5591341500764777]\n[-1.1511813581228247,0.5167912630953494]\n[-0.526616507590233,-1.9663658181161539]\n[-0.20046199002236306,0.24086423451083194]\n[0.1886857488192624,0.35570749672731583]\n[0.023857388832168525,1.32688027636613]\n[-0.545159873921753,-1.7646049316821777]\n[-0.46851671180781557,0.34953952684477685]\n[-0.13797128956682989,-1.679840571977491]\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\npredictedClusters: org.apache.spark.streaming.dstream.DStream[Int] = org.apache.spark.streaming.dstream.MappedDStream@427fc89c\n\nres12: scala.sys.ShutdownHookThread = Thread[shutdownHook2,5,main]\n\nsink: KafkaSink = KafkaSink@ad14eb9\n\nbroadcastedSink: org.apache.spark.broadcast.Broadcast[KafkaSink] = Broadcast(76)\n"}]},"apps":[],"jobName":"paragraph_1491333282716_1012628994","id":"20170323-211010_209261484","dateCreated":"2017-04-04T19:14:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1528","user":"anonymous","dateFinished":"2017-04-04T19:23:20+0000","dateStarted":"2017-04-04T19:23:19+0000"},{"text":"streamingContext.start()\nstreamingContext.awaitTermination()","dateUpdated":"2017-04-04T19:15:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"msg":[{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1491333282717_1012244245","id":"20170327-015215_1752989933","dateCreated":"2017-04-04T19:14:42+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1529"},{"dateUpdated":"2017-04-04T19:14:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491333282718_1013398492","id":"20170325-010254_615896214","dateCreated":"2017-04-04T19:14:42+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1530"}],"name":"FDP Sample Apps/SparkClustering","id":"2CE9R37R2","angularObjects":{"2CCJMKX7B:shared_process":[],"2CBZ4ZGED:shared_process":[],"2CFUBVKW9:shared_process":[],"2CET47A55:shared_process":[],"2CF7A3HGE:shared_process":[],"2CEP4S4GQ:shared_process":[],"2CF9VZ1G2:shared_process":[],"2CECY4PCV:shared_process":[],"2CCYJHV5M:shared_process":[],"2CCTATJPA:shared_process":[],"2CCUJN541:shared_process":[],"2CCRMT5VG:shared_process":[],"2CD4BQVNG:shared_process":[],"2CEER1PNP:shared_process":[],"2CEX7JJTK:shared_process":[],"2CC2E72E5:shared_process":[],"2CEU4QMPN:shared_process":[],"2CE5XNMXM:shared_process":[],"2CDSKBYG6:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}