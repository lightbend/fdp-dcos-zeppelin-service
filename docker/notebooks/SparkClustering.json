{"paragraphs":[{"text":"%md\nRun SparkClustering to iterate over the data set in micro batches and generate clustering information with anomalies\n\nUsage: Set the following parameters in the paragraph below by running nwintrusion/bin/app-install.sh --start-only anomaly-detection\nNO_OF_CLUSTERS\nTOPIC_TO_READ_FROM\nBROKER\nCLUSTER_TOPIC\nDURATION\n\nThis will run on streaming data from the specified Kafka topic in microbatches of the specified duration.\nThe result will tag every data point with a cluster number and also flag as anomaly for the anomalous data point.\nThe model for anomaly detection is based on the distance of the point from the nearest centroid being 3 standard\ndeviations away from the mean.","user":"anonymous","dateUpdated":"2017-04-04T19:22:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Run SparkClustering to iterate over the data set in micro batches and generate clustering information with anomalies</p>\n<p>Usage: Set the following parameters in the paragraph below by running nwintrusion/bin/app-install.sh &ndash;start-only anomaly-detection<br/>NO_OF_CLUSTERS<br/>TOPIC_TO_READ_FROM<br/>BROKER<br/>CLUSTER_TOPIC<br/>DURATION</p>\n<p>This will run on streaming data from the specified Kafka topic in microbatches of the specified duration.<br/>The result will tag every data point with a cluster number and also flag as anomaly for the anomalous data point.<br/>The model for anomaly detection is based on the distance of the point from the nearest centroid being 3 standard<br/>deviations away from the mean.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1491333282708_1015706985","id":"20170404-145214_264073144","dateCreated":"2017-04-04T19:14:42+0000","dateStarted":"2017-04-04T19:22:56+0000","dateFinished":"2017-04-04T19:22:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1408"},{"text":"import org.apache.spark.streaming.Seconds\n\n// Set the following 5 parameters\nval noOfClusters = <NO_OF_CLUSTERS>\nval topicToReadFrom = <TOPIC_TO_READ_FROM>\nval broker = <BROKER>\nval clusterTopic = <CLUSTER_TOPIC>\nval duration = <DURATION>","user":"anonymous","dateUpdated":"2017-04-05T21:52:57+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.streaming.Seconds\n\nnoOfClusters: Int = 30\n\ntopicToReadFrom: Array[String] = Array(nwout)\n\nbroker: String = broker-0.kafka.mesos:10036\n\nclusterTopic: String = nwcls\n\nduration: org.apache.spark.streaming.Duration = 1000 ms\n"}]},"apps":[],"jobName":"paragraph_1491333650751_-1026836270","id":"20170404-192050_1739523649","dateCreated":"2017-04-04T19:20:50+0000","dateStarted":"2017-04-05T21:52:57+0000","dateFinished":"2017-04-05T21:53:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1409"},{"text":"%spark\n\nimport org.apache.kafka.common.serialization.{ ByteArraySerializer, StringSerializer }\n\nimport org.apache.spark.mllib.linalg.{ Vectors, Vector }\nimport org.apache.spark.mllib.clustering.{ StreamingKMeans, StreamingKMeansModel }\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.StreamingContext\n\nimport java.io.Serializable\n\nimport org.apache.kafka.clients.producer.{ KafkaProducer, ProducerRecord, Callback, RecordMetadata }\n\nclass KafkaSink(createProducer: () => org.apache.kafka.clients.producer.KafkaProducer[Nothing, String]) extends Serializable {\n\n  lazy val producer = createProducer()\n\n  def send(topic: String, value: String): Unit = {\n    \n    val pr = new org.apache.kafka.clients.producer.ProducerRecord(topic, value)\n    val cb = new org.apache.kafka.clients.producer.Callback {\n      override def onCompletion(metadata: org.apache.kafka.clients.producer.RecordMetadata, ex: Exception) = \n        if (ex != null) ex.printStackTrace\n    }\n\n    producer.send(pr, cb)\n  }\n}\n\ndef createKafkaSink(broker: String) = {\n    val kafkaParams = new java.util.HashMap[String, Object]()\n    kafkaParams.put(\"key.serializer\", classOf[org.apache.kafka.common.serialization.ByteArraySerializer])\n    kafkaParams.put(\"value.serializer\", classOf[org.apache.kafka.common.serialization.StringSerializer])\n    kafkaParams.put(\"bootstrap.servers\", broker)\n    \n    val f = () => {\n       val producer = new org.apache.kafka.clients.producer.KafkaProducer[Nothing, String](kafkaParams)\n\n       sys.addShutdownHook {\n         producer.close()\n       }\n\n       producer\n     }\n     new KafkaSink(f)\n}\n\nobject KafkaPublisher extends Serializable {\n  /**\n   * Predict cluster from `labeledData` and publish to Kafka. \n   *\n   * The following elements are published separated by comma:\n   *\n   * a. predicted cluster number\n   * b. centroid of the cluster\n   * c. distance of the point from the centroid\n   * d. the label of the point\n   * e. if anomalous\n   * f. the point vector as a comma separated string\n   *\n   * For every microbatch, a separate record is written to the same topic, which contains\n   * the pattern : \"Centroids:/<centroid1>/<centroid2>/...\"\n   */ \n\n  def writeToKafka(labeledData: DStream[(String, org.apache.spark.mllib.linalg.Vector)], skmodel:  org.apache.spark.mllib.clustering.StreamingKMeansModel, sink: KafkaSink, clusterTopic: String) = labeledData.foreachRDD { rdd =>\n\n    var currentHundredthFarthest: Double = 0.0d\n    if (rdd.count() > 0) {\n  \n      // for the RDD of the microbatch, get the mean distance to centroid for all the Vectors\n      val distancesToCentroid: org.apache.spark.rdd.RDD[(Int, Double)] = rdd.map { case (l, v) => \n        val (cluster, _, dist) = distanceToCentroid(skmodel, v)\n        (cluster, dist)\n      }\n\n      /**\n       * We compute the hundredth farthest point from centroid in each micro batch and\n       * delcare a data point anomalous if it's distance from the nearest centroid exceeds this value.\n       * We maintain the highest of the values that we get for each batch. This is not entirely\n       * foolproof a model, but this will improve with more iterations\n       */ \n      val hundredthFarthestDistanceToCentroid: Double = { \n        val t100 = rdd.map { case (l, v) => distanceToCentroid(skmodel, v)._3 }.top(100)\n        if (t100.isEmpty) 0.00 else t100.last\n      }\n      if (hundredthFarthestDistanceToCentroid > currentHundredthFarthest) \n        currentHundredthFarthest = hundredthFarthestDistanceToCentroid\n\n      (rdd.zipWithIndex).foreach { case ((label, vec), idx) =>\n  \n        // for each Vector in the RDD get the cluster membership, centroid and distance to centroid\n        val (predictedCluster, centroid, distanceToCentroidForVec) = distanceToCentroid(skmodel, vec) \n  \n        // anomalous if its distance is more than the hundredth farthest distance\n        // This strategy is taken from the book Advanced Analytics with Spark (http://shop.oreilly.com/product/0636920035091.do)\n        val isAnomalous = distanceToCentroidForVec > currentHundredthFarthest\n  \n        val message = \n          s\"\"\"$predictedCluster,$centroid,$distanceToCentroidForVec,$label,$isAnomalous,${vec.toArray.mkString(\",\")}\"\"\"\n  \n        // write the centroid record only for the first item in the microbatch\n        if (idx == 0) sink.send(clusterTopic, createClusterCenterRecord(skmodel))\n        sink.send(clusterTopic, message)\n      }\n    }\n  }\n  \n  def publishClusterInfoToKafka(labeledData: DStream[(String, org.apache.spark.mllib.linalg.Vector)], model: org.apache.spark.mllib.clustering.StreamingKMeans, sink: KafkaSink, clusterTopic: String, broker: String) = {\n    writeToKafka(labeledData, model.latestModel, sink, clusterTopic)\n  }\n\n  /**\n   * Create a record consisting of cluster centroids separated by \"/\". The cluster centroids are in the \n   * order of the cluster numbers.\n   */ \n  def createClusterCenterRecord(skmodel:  org.apache.spark.mllib.clustering.StreamingKMeansModel): String = {\n    val centers: Array[org.apache.spark.mllib.linalg.Vector] = skmodel.clusterCenters\n    centers.foldLeft(s\"Centroids:\") { (a, e) =>\n      s\"$a/${e.toArray.mkString(\",\")}\"\n    }\n  }\n\n  private def distanceToCentroid(skmodel:  org.apache.spark.mllib.clustering.StreamingKMeansModel, vec: org.apache.spark.mllib.linalg.Vector): (Int, org.apache.spark.mllib.linalg.Vector, Double) = {\n    val predictedCluster: Int = skmodel.predict(vec)\n    val centroid: org.apache.spark.mllib.linalg.Vector = skmodel.clusterCenters(predictedCluster)\n    (predictedCluster, centroid, org.apache.spark.mllib.linalg.Vectors.sqdist(centroid, vec))\n  }\n\n  private def normalize(rdd: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)]): org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] = {\n    val labels: org.apache.spark.rdd.RDD[String] = rdd.map(_._1)\n    val data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = rdd.map(_._2)\n    labels.zip(new org.apache.spark.mllib.feature.StandardScaler().fit(data).transform(data))\n  }\n}\n","user":"anonymous","dateUpdated":"2017-04-05T21:53:29+0000","config":{"lineNumbers":true,"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.kafka.common.serialization.{ByteArraySerializer, StringSerializer}\n\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\n\nimport org.apache.spark.mllib.clustering.{StreamingKMeans, StreamingKMeansModel}\n\nimport org.apache.spark.mllib.feature.StandardScaler\n\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.streaming.dstream.DStream\n\nimport org.apache.spark.streaming.StreamingContext\n\nimport java.io.Serializable\n\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, Callback, RecordMetadata}\n\ndefined class KafkaSink\n\n\ncreateKafkaSink: (broker: String)KafkaSink\ndefined object KafkaPublisher\n"}]},"apps":[],"jobName":"paragraph_1491333282713_1013783240","id":"20170324-014043_420574488","dateCreated":"2017-04-04T19:14:42+0000","dateStarted":"2017-04-05T21:53:29+0000","dateFinished":"2017-04-05T21:53:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1410"},{"text":"import org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.{ ByteArrayDeserializer, StringDeserializer}\n\nimport org.apache.spark.mllib.clustering.{ StreamingKMeans, StreamingKMeansModel }\nimport org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.mllib.linalg.{ Vector, Vectors }\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n\nimport java.nio.charset.Charset\n\ndef projectToLowerDimension: RDD[(String, Vector)] => RDD[(String, Vector)] = { rdd =>\n    if (rdd.isEmpty) rdd else {\n      // reduce to 2 dimensions\n      val pca = new PCA(2).fit(rdd.map(_._2))\n    \n      // Project vectors to the linear space spanned by the top 2 principal\n      // components, keeping the label\n      rdd.map(p => (p._1, pca.transform(p._2)))\n    }\n}\n\ndef normalize: RDD[(String, Vector)] => RDD[(String, Vector)] = { rdd =>\n    if (rdd.isEmpty) rdd else {\n      val labels: RDD[String] = rdd.map(_._1)\n      val data: RDD[Vector] = rdd.map(_._2)\n      labels.zip(new StandardScaler().fit(data).transform(data))\n    }\n}\n\nval streamingContext = new StreamingContext(sc, duration)\n// streamingContext.checkpoint(\"checkpoint\")\n\nval kafkaParams = Map[String, Object](\n  \"bootstrap.servers\"    -> broker,\n  \"key.deserializer\"     -> classOf[ByteArrayDeserializer],\n  \"value.deserializer\"   -> classOf[ByteArrayDeserializer],\n  \"group.id\"             -> s\"example${System.currentTimeMillis}\",\n  \"auto.offset.reset\"    -> \"latest\",\n  \"enable.auto.commit\"   -> (true: java.lang.Boolean)\n)\n\n// get the data from kafka\nval stream: DStream[ConsumerRecord[Array[Byte], Array[Byte]]] = \n  KafkaUtils.createDirectStream[Array[Byte], Array[Byte]](\n    streamingContext,\n    PreferConsistent,\n    Subscribe[Array[Byte], Array[Byte]](topicToReadFrom, kafkaParams)\n  )\n\n// label and vectorize the value\nval transformed: DStream[(String, Vector)] = stream.map { record =>\n  val bytes: Array[Byte] = record.value\n  val CHARSET = Charset.forName(\"UTF-8\")\n  val arr = new String(bytes, CHARSET).split(\"/\")\n  val (label, value) = (arr(0), arr(1))\n  val vector = Vectors.dense(value.split(\",\").map(_.toDouble))\n  (label, vector)\n}\n\ntransformed.cache()\n\n// normalize\nval normalized: DStream[(String, Vector)] = transformed.transform(normalize)\n\n// project to lower dimension\nval projected: DStream[(String, Vector)] = normalized.transform(projectToLowerDimension)\n\n// get data only\nval data: DStream[Vector] = projected.map(_._2)\n","user":"anonymous","dateUpdated":"2017-04-05T21:53:34+0000","config":{"lineNumbers":true,"tableHide":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.kafka.clients.consumer.ConsumerRecord\n\nimport org.apache.kafka.common.serialization.{ByteArrayDeserializer, StringDeserializer}\n\nimport org.apache.spark.mllib.clustering.{StreamingKMeans, StreamingKMeansModel}\n\nimport org.apache.spark.mllib.feature.PCA\n\nimport org.apache.spark.mllib.feature.StandardScaler\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.SparkConf\n\nimport org.apache.spark.streaming.kafka010._\n\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n\nimport java.nio.charset.Charset\n\nprojectToLowerDimension: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] => org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)]\n\nnormalize: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] => org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)]\n\nstreamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@1ecf96cc\n\nkafkaParams: scala.collection.immutable.Map[String,Object] = Map(key.deserializer -> class org.apache.kafka.common.serialization.ByteArrayDeserializer, auto.offset.reset -> latest, group.id -> example1491429216633, bootstrap.servers -> broker-0.kafka.mesos:10036, enable.auto.commit -> true, value.deserializer -> class org.apache.kafka.common.serialization.ByteArrayDeserializer)\n\nstream: org.apache.spark.streaming.dstream.DStream[org.apache.kafka.clients.consumer.ConsumerRecord[Array[Byte],Array[Byte]]] = org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@235bdf7f\n\ntransformed: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)] = org.apache.spark.streaming.dstream.MappedDStream@5bf047a4\n\nres0: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)] = org.apache.spark.streaming.dstream.MappedDStream@5bf047a4\n\nnormalized: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)] = org.apache.spark.streaming.dstream.TransformedDStream@2755f699\n\nprojected: org.apache.spark.streaming.dstream.DStream[(String, org.apache.spark.mllib.linalg.Vector)] = org.apache.spark.streaming.dstream.TransformedDStream@48e43bb0\n\ndata: org.apache.spark.streaming.dstream.DStream[org.apache.spark.mllib.linalg.Vector] = org.apache.spark.streaming.dstream.MappedDStream@7df2db0e\n"}]},"apps":[],"jobName":"paragraph_1491333282715_1014552738","id":"20170323-205432_560258814","dateCreated":"2017-04-04T19:14:42+0000","dateStarted":"2017-04-05T21:53:34+0000","dateFinished":"2017-04-05T21:53:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1411"},{"text":"val decayFactor = 1      // assume stationary distribution\nval dimensions = 2       // our data is now a 2 dimension vector\nval weightPerCenter = 0.0\n\nval model = new StreamingKMeans()\n  .setDecayFactor(decayFactor)\n  .setK(noOfClusters)\n  .setRandomCenters(dimensions, weightPerCenter)\n\nmodel.trainOn(data)\n\nval skmodel = model.latestModel\n\nskmodel.clusterCenters.foreach(println)\nskmodel.clusterWeights.foreach(println)\n\n// cluster distribution on prediction\nval predictedClusters: DStream[Int] = model.predictOn(data)\n\n// prints counts per cluster \npredictedClusters.countByValue().print()\n\npredictedClusters.foreachRDD { rdd => println(rdd.count()) }\n\nsys.ShutdownHookThread {\n  streamingContext.stop(stopSparkContext=false, stopGracefully=true)\n}\n\nval sink = createKafkaSink(broker)\nval broadcastedSink = streamingContext.sparkContext.broadcast(sink)\n// publish cluster info to a kafka topic\nKafkaPublisher.publishClusterInfoToKafka(projected, model, broadcastedSink.value, clusterTopic, broker)","user":"anonymous","dateUpdated":"2017-04-05T21:53:40+0000","config":{"tableHide":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndecayFactor: Int = 1\n\ndimensions: Int = 2\n\nweightPerCenter: Double = 0.0\n\nmodel: org.apache.spark.mllib.clustering.StreamingKMeans = org.apache.spark.mllib.clustering.StreamingKMeans@56f39091\n\nskmodel: org.apache.spark.mllib.clustering.StreamingKMeansModel = org.apache.spark.mllib.clustering.StreamingKMeansModel@71570c0\n[-1.0372469553734758,-0.1255301782091715]\n[1.6717418680797016,-0.4626174469317164]\n[-0.07113632767341853,0.3175200082478478]\n[-0.3097015150283481,-0.1837013664984934]\n[-3.041758479090009,0.5308084611108314]\n[1.8222021926881693,0.6126940025773624]\n[-1.3031603439687232,0.8208063882787223]\n[0.9999523626265802,-0.8129028096734997]\n[-2.1051260249896764,1.5695782470637016]\n[-0.169502548070545,-0.018083687739369227]\n[0.5563426360867698,0.48910909104876116]\n[0.18229118128029267,-0.930926073932529]\n[-0.4154128480125845,0.5796326929172827]\n[-0.65799490023372,0.14629669304172654]\n[0.8369068658415482,-0.44541418384377135]\n[3.4057707004655735,-0.4979082275706592]\n[0.4831431674767794,0.8338180657133909]\n[-0.4565946000093053,1.081463616857949]\n[0.3371703850997155,-1.36049977358226]\n[0.27993779780791495,0.4779239092290087]\n[0.6418442576455865,0.5214049318028205]\n[-2.66129783648894,0.8628964774201944]\n[-0.3949773530568215,0.9741382565709511]\n[-1.4548124788907426,0.5644995420223606]\n[1.0550728208647797,-0.9253126542912509]\n[-0.26949216608615467,-0.2932519514229515]\n[-0.5129377151905574,0.29563382403436983]\n[1.3282066128978205,1.9159274351169113]\n[0.5430629004925517,0.7350482399928866]\n[0.7529286652885484,0.19965361472711607]\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\npredictedClusters: org.apache.spark.streaming.dstream.DStream[Int] = org.apache.spark.streaming.dstream.MappedDStream@1d3b4f18\n\nres6: scala.sys.ShutdownHookThread = Thread[shutdownHook1,5,main]\n\nsink: KafkaSink = KafkaSink@231bca91\n\nbroadcastedSink: org.apache.spark.broadcast.Broadcast[KafkaSink] = Broadcast(0)\n"}]},"apps":[],"jobName":"paragraph_1491333282716_1012628994","id":"20170323-211010_209261484","dateCreated":"2017-04-04T19:14:42+0000","dateStarted":"2017-04-05T21:53:40+0000","dateFinished":"2017-04-05T21:53:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1412"},{"text":"streamingContext.start()\nstreamingContext.awaitTermination()","dateUpdated":"2017-04-05T21:58:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{"0":{"graph":{"mode":"table","height":301,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"msg":[{"data":"","type":"TEXT"}]},"apps":[],"jobName":"paragraph_1491333282717_1012244245","id":"20170327-015215_1752989933","dateCreated":"2017-04-04T19:14:42+0000","status":"RUNNING","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1413","user":"anonymous","dateStarted":"2017-04-05T21:54:01+0000"},{"dateUpdated":"2017-04-04T19:14:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491333282718_1013398492","id":"20170325-010254_615896214","dateCreated":"2017-04-04T19:14:42+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1414"}],"name":"FDP Sample Apps/SparkClustering","id":"2CE9R37R2","angularObjects":{"2CCBAM1E2:shared_process":[],"2CEC1EDW6:shared_process":[],"2CDBV3JNN:shared_process":[],"2CD8EJ7NH:shared_process":[],"2CCKJC2R2:shared_process":[],"2CD5Q2QUR:shared_process":[],"2CCWSPX42:shared_process":[],"2CF34TCFQ:shared_process":[],"2CFMX3SV8:shared_process":[],"2CD6Q7CHF:shared_process":[],"2CCV9KQQN:shared_process":[],"2CF5XH8U3:shared_process":[],"2CC7T8WYE:shared_process":[],"2CFVN9FD5:shared_process":[],"2CFSVFNNN:shared_process":[],"2CCK7W8PG:shared_process":[],"2CEQJVYAJ:shared_process":[],"2CECK9WU3:shared_process":[],"2CDCWFJSK:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}